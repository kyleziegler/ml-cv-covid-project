{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ac519b0-ff51-49eb-8cb8-78213a72b7fe",
   "metadata": {},
   "source": [
    "# Computer Vision, Image Classification - Covid CT Scans\n",
    "Kyle Ziegler 4/2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b170cdda-1f16-4480-8404-829c59a8673f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd51ea85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Set Parameters\"\"\"\n",
    "\n",
    "# Model\n",
    "mode = 'transfer_learning'\n",
    "num_classes = 4\n",
    "\n",
    "# Data\n",
    "data_path = 'gs://vertex-central-1f/covid_proj_tfrecords/train/TFRECORD*'\n",
    "num_examples = 195000\n",
    "loop_dataset = -1 # Use -1 to create infinite dataset\n",
    "image_channels = 3\n",
    "height, width = (300,300)\n",
    "\n",
    "# Training\n",
    "epochs = 100\n",
    "batch_size = 256\n",
    "steps_per_epoch = num_examples//batch_size\n",
    "\n",
    "distribution_strategy = \"mirrored\" # Use mirrored or multi-worker mirrored\n",
    "\n",
    "initial_learning_rate = 0.1\n",
    "decay_steps = 100000\n",
    "decay_rate = 0.96\n",
    "\n",
    "# Logging\n",
    "tb_resource_name = 'projects/156596422468/locations/us-central1/tensorboards/3919064061672685568'\n",
    "log_dir = os.getcwd() + '/fit/' + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "save_model_path = os.getcwd() + '/saved_models/' + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "histogram_freq = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb4c34a",
   "metadata": {},
   "source": [
    "### Distributed Training Setup\n",
    "- Single machine with n number of GPUs, can be adjusted to anything in the [TF docs](https://www.tensorflow.org/guide/distributed_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d8327f-451c-47bd-9c60-c6a0421f4049",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import mixed_precision\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "if distribution_strategy == \"mirrored\":\n",
    "    distribution_strategy = tf.distribute.MirroredStrategy()\n",
    "else:\n",
    "    distribution_strategy = tf.distribute.MultiWorkerMirroredStrategy()\n",
    "\n",
    "with distribution_strategy.scope():\n",
    "    def create_model():\n",
    "\n",
    "        input_shape = (height, width, image_channels)\n",
    "        input_layer = tf.keras.layers.Input(input_shape)\n",
    "\n",
    "        # Base\n",
    "        # base_layers = layers.experimental.preprocessing.Rescaling(1./255, name='bl_1')(input_layer)\n",
    "        base_layers = layers.Conv2D(16, 3, padding='same', activation='relu', name='bl_2')(input_layer)\n",
    "        base_layers = layers.MaxPooling2D(name='bl_3')(base_layers)\n",
    "        base_layers = layers.Conv2D(32, 3, padding='same', activation='relu', name='bl_4')(base_layers)\n",
    "        base_layers = layers.MaxPooling2D(name='bl_5')(base_layers)\n",
    "        base_layers = layers.Conv2D(64, 3, padding='same', activation='relu', name='bl_6')(base_layers)\n",
    "        base_layers = layers.MaxPooling2D(name='bl_7')(base_layers)\n",
    "        base_layers = layers.Flatten(name='bl_8')(base_layers)\n",
    "\n",
    "        # Classifier branch\n",
    "        classifier_branch = layers.Dense(128, activation='relu', name='cl_1')(base_layers)\n",
    "        classifier_branch = layers.Dense(num_classes, name='cl_head')(classifier_branch)\n",
    "        # logisitic regression for each possible class\n",
    "\n",
    "        # Localizer branch\n",
    "        locator_branch = layers.Dense(128, activation='relu', name='bb_1')(base_layers)\n",
    "        locator_branch = layers.Dense(64, activation='relu', name='bb_2')(locator_branch)\n",
    "        locator_branch = layers.Dense(32, activation='relu', name='bb_3')(locator_branch)\n",
    "        locator_branch = layers.Dense(4, activation='sigmoid', name='bb_head')(locator_branch)\n",
    "        # output 4 floats, MSE loss metric\n",
    "\n",
    "        model = tf.keras.Model(input_layer, outputs=[classifier_branch,locator_branch])\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def create_transfer_learning_model():\n",
    "        \n",
    "        base_model = tf.keras.applications.ResNet101V2(\n",
    "            include_top=False,\n",
    "            weights=\"imagenet\",\n",
    "            input_shape=(height, width, image_channels),\n",
    "            pooling=\"avg\",\n",
    "            classifier_activation=None, # only used when you are including the top\n",
    "        )\n",
    "        base_model.trainable = False\n",
    "        \n",
    "        input_shape = (height, width, image_channels)\n",
    "        input_layer = tf.keras.layers.Input(input_shape)\n",
    "        \n",
    "        base_layers = base_model(input_layer, training=False)\n",
    "        \n",
    "        # base_model.add(Flatten())\n",
    "        # flatten = tf.keras.layers.Flatten()(base_model.layers[-1].output)\n",
    "        \n",
    "        # Classifier branch\n",
    "        classifier_branch = layers.Dense(128, activation='relu', name='cl_1')(base_layers)\n",
    "        classifier_branch = layers.Dense(num_classes, name='cl_head')(classifier_branch)\n",
    "        # logisitic regression for each possible class\n",
    "\n",
    "        # Localizer branch\n",
    "        locator_branch = layers.Dense(128, activation='relu', name='bb_1')(base_layers)\n",
    "        locator_branch = layers.Dense(64, activation='relu', name='bb_2')(locator_branch)\n",
    "        locator_branch = layers.Dense(32, activation='relu', name='bb_3')(locator_branch)\n",
    "        locator_branch = layers.Dense(4, activation='sigmoid', name='bb_head')(locator_branch)\n",
    "        # output 4 floats, MSE loss metric\n",
    "\n",
    "        model = tf.keras.Model(input_layer, outputs=[classifier_branch,locator_branch])\n",
    "        return model\n",
    "\n",
    "    if mode == 'transfer_learning':\n",
    "        model = create_transfer_learning_model()\n",
    "    else:\n",
    "        model = create_model()\n",
    "    \n",
    "# Creates a png of the network\n",
    "# tf.keras.utils.plot_model(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136e1105-0dee-4edb-8db0-e48b6e299e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa416d13-1053-4216-b272-3c9c04e54905",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = { \n",
    "    \"cl_head\":tf.keras.losses.SparseCategoricalCrossentropy(),        \n",
    "    \"bb_head\":tf.keras.losses.MSE\n",
    "}\n",
    "\n",
    "metrics = { \n",
    "    \"cl_head\": \"accuracy\",\n",
    "    \"bb_head\": \"mean_squared_error\"\n",
    "}\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=decay_steps,\n",
    "    decay_rate=decay_rate,\n",
    "    staircase=True)\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "model.compile(loss=losses, optimizer=opt, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169fa0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing mode object\n",
    "import model\n",
    "\n",
    "# Allows model file to be adjusted, and will be recompiled with each \n",
    "# execution of this cell.\n",
    "from importlib import reload \n",
    "reload(model)\n",
    "\n",
    "from model import Model\n",
    "\n",
    "model = Model((height, width), num_classes, image_channels)\n",
    "model.create_model(\"transfer_learning\",\"mirrored\")\n",
    "model.add_loss_function(initial_learning_rate, decay_steps, decay_rate)\n",
    "\n",
    "m = model.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3733f3-c242-47fe-8707-ee93a5832bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_record(record):\n",
    "    \"\"\"Parse a single record, and create TF features\"\"\"\n",
    "\n",
    "    feature_mapping = {\n",
    "        'image': tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n",
    "        'bounding_box': tf.io.FixedLenFeature([4], tf.float32),\n",
    "        'target_class': tf.io.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "    \n",
    "    file_rec = tf.io.parse_single_example(record, feature_mapping)\n",
    "    \n",
    "    image = file_rec[\"image\"]\n",
    "    image = tf.io.decode_png(image, channels=image_channels)\n",
    "    image = tf.image.resize(image, (height, width))\n",
    "    \n",
    "    # image = tf.image.convert_image_dtype(image, tf.float32) # normalizes between [0,1]\n",
    "\n",
    "    # Best practice to use per image standardization, results in higher performance as well\n",
    "    image = tf.image.per_image_standardization(image) \n",
    "    print(image)\n",
    "    \n",
    "    # print(image)\n",
    "    \n",
    "    bounding_box = file_rec[\"bounding_box\"]\n",
    "    \n",
    "    target_class = file_rec[\"target_class\"]\n",
    "    \n",
    "    return image, (target_class, bounding_box)\n",
    "\n",
    "def create_prefetch_dataset(file_path):\n",
    "    \"\"\"Create the tf.dataset object, used to feed into fit() function\"\"\"\n",
    "    \n",
    "    print('Creating Prefetch Dataset From', file_path)\n",
    "\n",
    "    # list_files have shuffle True by default\n",
    "    dataset = tf.data.Dataset.list_files(file_path)\n",
    "    \n",
    "    # A note on caching with cache(), you should only use this on small datasets \n",
    "    # that fit into memory, otherwise you'll crash your machine. Don't ask how I know.\n",
    "    \n",
    "    dataset = dataset.interleave(tf.data.TFRecordDataset, num_parallel_calls=tf.data.AUTOTUNE)\\\n",
    "    .map(_parse_record, num_parallel_calls=tf.data.AUTOTUNE)\\\n",
    "    .shuffle(3, reshuffle_each_iteration=False)\\\n",
    "    .batch(batch_size, drop_remainder=True)\\\n",
    "    .prefetch(tf.data.AUTOTUNE)\\\n",
    "    .repeat(loop_dataset)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "dataset = create_prefetch_dataset(data_path)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d33d93b-b8bc-4ef1-a95a-54e449db5533",
   "metadata": {},
   "source": [
    "### Notes on performance\n",
    "- When using a batch size of 256, main mem is around 15GB used, slightly increasing over each epoch.\n",
    "- CPU is around 10 cores at 100%, fluctuating between 5-11 cores.\n",
    "- GPU is around 75-95% utilization, only dropping for a second between epochs. GPU mem is around 15GB on a 16GB card."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140b6d0a-00d3-479a-b8f2-10c0f01406fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for writing profile metrics, does not work in Vertex hosted Tensorboard 4/2022\n",
    "# tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir,\n",
    "#                                                       histogram_freq=1,\n",
    "#                                                       profile_batch = '0,10',\n",
    "#                                                       write_images=False\n",
    "#                                                      )\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir,\n",
    "                                                      histogram_freq=histogram_freq,\n",
    "                                                     update_freq='epoch')\n",
    "    \n",
    "history = model.fit(dataset, epochs=epochs, steps_per_epoch=steps_per_epoch, initial_epoch = 0, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0450e9-a4bd-4644-8e38-2fbdb173b77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "tb-gcp-uploader --tensorboard_resource_name \\\n",
    "  tb_resource_name \\\n",
    "  --logdir=log_dir \\\n",
    "  --experiment_name=model-fit --one_shot=True"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m91"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
